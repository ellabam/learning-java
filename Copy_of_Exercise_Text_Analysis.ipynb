{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ellabam/learning-java/blob/master/Copy_of_Exercise_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaPzkjZuHFVT"
      },
      "outputs": [],
      "source": [
        "import numpy as np  #load up the libraries and object defs. we need\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "\n",
        "# load up my visualization system, and call the object plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# tell ipython notebook to print visualizations inline\n",
        "%pylab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6eoNgybHFVW"
      },
      "source": [
        "# NLTK\n",
        "\n",
        "Import the nltk (Natural Language Toolkit) package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDXO4oqZHFVX"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjoJ9yPgHFVY"
      },
      "source": [
        "NTLK doesn't contain many language packages by default, so you'll need to install a few. If you haven't installed the `punkt` tokenizer package, you'll need to uncomment and run the line below to launch the NLTK downloader. \n",
        "\n",
        "Once it's loaded, enter `d` and then download the `punkt` package.\n",
        "\n",
        "While you're there. Also download the `averaged_perceptron_tagger`, `stopwords`, and `vader_lexicon` packages. (We'll use them later.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh6Vd9YSHFVY"
      },
      "outputs": [],
      "source": [
        "nltk.download_shell()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNfqDvzDHFVY"
      },
      "source": [
        "## Text Splitting Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47JwRaZgHFVZ"
      },
      "outputs": [],
      "source": [
        "s = \"This is a short, simple piece of text, so let's try a short, simple analysis.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdfdOQJsHFVZ"
      },
      "source": [
        "Breaking it up with `string.split()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6kL3UZUHFVa"
      },
      "outputs": [],
      "source": [
        "s.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH3v6_FuHFVa"
      },
      "outputs": [],
      "source": [
        "s.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppgNkjVLHFVb"
      },
      "source": [
        "## Tokenizing\n",
        "Using NLTK, we can split (\"tokenize\") more intelligently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CLSz3VlHFVb"
      },
      "outputs": [],
      "source": [
        "nltk.wordpunct_tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-8rbEsrHFVb"
      },
      "source": [
        "We can also count and visualize the distribution of terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnAOnFm-HFVb"
      },
      "outputs": [],
      "source": [
        "tokens = nltk.wordpunct_tokenize(s)\n",
        "nltk.probability.FreqDist(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su5b5CrTHFVc"
      },
      "outputs": [],
      "source": [
        "fd = nltk.probability.FreqDist(tokens)\n",
        "fd.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe4eS24HFVc"
      },
      "source": [
        "### Removing stop words\n",
        "NLTK provides corpuses of common stop words that we can use to remove common terms from text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA8lmu5KHFVc"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B8zYR6cHFVc"
      },
      "source": [
        "Here's a slow way to remove stop words using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlThxpy2HFVc"
      },
      "outputs": [],
      "source": [
        "t = s.lower()   # make lowercase\n",
        "tokens = nltk.tokenize.wordpunct_tokenize(t)\n",
        "no_stopwords = []  # this will hold our NON-stopword words\n",
        "for t in tokens:   # for each word\n",
        "    if t not in stopwords:  # not in stopword list\n",
        "        no_stopwords.append(t)  # collect the word\n",
        "\n",
        "print(no_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9DAZthsHFVd"
      },
      "source": [
        "#### A nice Python shortcut\n",
        "You can actually do this kind of transformation more concisely by using list comprehensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8KxkUBLHFVd"
      },
      "outputs": [],
      "source": [
        "tokens = nltk.wordpunct_tokenize(s)\n",
        "print(tokens)   # non-lowercase version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCrwVo-qHFVd"
      },
      "outputs": [],
      "source": [
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# ^ this list comprenehsion is basically a shorthand for:\n",
        "#lower_tokens = []\n",
        "#for t in tokens:\n",
        "#    lower_tokens.append(t.lower())\n",
        "print(lower_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAJUvrG9HFVd"
      },
      "source": [
        "So to remove stopwords, we can nest an if statement inside the list comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOFUMJBeHFVe"
      },
      "outputs": [],
      "source": [
        "tokens_nostop = [t.lower() for t in tokens if t not in stopwords]\n",
        "print(tokens_nostop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vRqljDuHFVe"
      },
      "source": [
        "You can remove punctuation the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcgLXP75HFVe"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbzHjpTAHFVe"
      },
      "outputs": [],
      "source": [
        "tokens_nopunct = [t.lower() for t in tokens if t not in string.punctuation]\n",
        "print(tokens_nopunct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMF3s__LHFVe"
      },
      "source": [
        "Or we can do both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of8NvAMsHFVe"
      },
      "outputs": [],
      "source": [
        "tokens_nostop_nopunct = [t.lower() for t in tokens if (t not in string.punctuation and t not in stopwords)]\n",
        "print(tokens_nostop_nopunct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1bPLyV0HFVe"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDNmCz7KHFVf"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.snowball.EnglishStemmer()  # grab me a \"snowball stemmer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPo0MR-EHFVf"
      },
      "outputs": [],
      "source": [
        "stemmer.stem(\"runs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcRP9dkIHFVf"
      },
      "outputs": [],
      "source": [
        "stemmer.stem(\"applicable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtI_V1gNHFVf"
      },
      "source": [
        "Lowercase, remove stopwords, and stem all in one line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSAtYGLvHFVf"
      },
      "outputs": [],
      "source": [
        "filtered = [stemmer.stem(t.lower()) for t in tokens if (t not in string.punctuation and t.lower() not in stopwords)]\n",
        "print(filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC6LXW3KHFVf"
      },
      "outputs": [],
      "source": [
        "print(s)\n",
        "nltk.probability.FreqDist(filtered).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Ps8LMmHFVf"
      },
      "source": [
        "### Parts of Speech\n",
        "Labels each term in the text with its part of speech (noun, verb, adjective, etc.). \n",
        "\n",
        "Parts of speech are labeled using the codes from the UPENN Treebank project: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mfi5THFHFVg"
      },
      "outputs": [],
      "source": [
        "nltk.pos_tag(tokens_nopunct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GarhRNciHFVg"
      },
      "outputs": [],
      "source": [
        "pos_list = [i[1] for i in nltk.pos_tag(tokens_nopunct)]\n",
        "nltk.probability.FreqDist(pos_list).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNT8qnntHFVg"
      },
      "source": [
        "### n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNGNF6prHFVg"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "[i for i in nltk.ngrams(tokens_nopunct,n)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hsuKK55HFVg"
      },
      "source": [
        "### Sentiment\n",
        "Sentiment analysis can be interesting, but it's also **very** inexact. You should always take it with a grain of salt, especially on small corpuses of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCLtGeYVHFVg"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPEkF7PWHFVg"
      },
      "outputs": [],
      "source": [
        "print(s)\n",
        "sid.polarity_scores(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9b5eUYfHFVh"
      },
      "source": [
        "^ The compound score here combines the others and will for negative sentences/phrases deemed to have negative sentiment and positive for text deemed to have positive sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcONZ-HIHFVh"
      },
      "outputs": [],
      "source": [
        "sid.polarity_scores(\"The movie was atrocious with horrible acting and a deeply flawed plot.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FjShnOrHFVh"
      },
      "outputs": [],
      "source": [
        "sid.polarity_scores(\"The movie was amazing with stellar acting and an incredible plot.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27j8PXthHFVh"
      },
      "source": [
        "## Tweet data\n",
        "Let's try this on a small sample of ~300 tweets from [@BoredElonMusk](https://twitter.com/BoredElonMusk?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMnhF649HFVh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=15J4CzqPEVVkPbaFUSko288_ppcXJG4hE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0thtziZHFVh"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM13aAF9HFVh"
      },
      "source": [
        "### Tweet length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wP4ZpIMHFVh"
      },
      "outputs": [],
      "source": [
        "df['tweet-length'] = [len(t) for t in df['tweet-text']]\n",
        "print(df.head(5))\n",
        "df['tweet-length'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsM69LfxHFVh"
      },
      "source": [
        "### Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOsUtvEsHFVi"
      },
      "outputs": [],
      "source": [
        "df['sentiment'] = df['tweet-text'].map(lambda x: sid.polarity_scores(x)[\"compound\"])\n",
        "print(df[['tweet-text','sentiment']].head(5))\n",
        "df['sentiment'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9xxNNZTHFVi"
      },
      "source": [
        "### Word frequency\n",
        "Tokenize the tweet text and remove punctuation and stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okweHAtVHFVi"
      },
      "outputs": [],
      "source": [
        "df['tweet-tokens'] = df['tweet-text'].map(lambda x: nltk.wordpunct_tokenize(x))\n",
        "df[['tweet-text','tweet-tokens']].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZmSkvGzHFVi"
      },
      "outputs": [],
      "source": [
        "def remove_stop_punct(token_list):\n",
        "    return [t.lower() for t in token_list if (t not in string.punctuation and t.lower() not in stopwords)]\n",
        "\n",
        "df['tweet-tokens'] = [nltk.wordpunct_tokenize(tweet) for tweet in df['tweet-text']]\n",
        "df['tweet-tokens-filtered'] = df['tweet-tokens'].map(remove_stop_punct)\n",
        "df.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_VB_x3GHFVi"
      },
      "source": [
        "Get a list of all of the tokens in the whole corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFPBPmpXHFVi"
      },
      "outputs": [],
      "source": [
        "tweet_tokens_filtered_list = [item for sublist in df[\"tweet-tokens-filtered\"] for item in sublist]\n",
        "nltk.FreqDist(tweet_tokens_filtered_list).plot(30) #Show me just the top 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8oJHeQVHFVi"
      },
      "source": [
        "# ToDo\n",
        "Are there terms that should be removed from this frequency distribution? Think about how you'd remove them.\n",
        "\n",
        "*Hint: Think about how we removed punctuation and stop words earlier.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2i9qa7OHFVi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of Exercise - Text Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}